#!/usr/bin/env python3
"""
from playwright.async_api import async_playwright

Investigative script –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ Kufar city lookup.
–°–æ–±–∏—Ä–∞–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç HAR/–ª–æ–≥–∏/HTML, —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç endpoints –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç—ã.
"""
import os
import sys
import json
import re
import time
import asyncio
import logging
import aiohttp
import zipfile
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from urllib.parse import urlparse, parse_qs, urlencode
from collections import defaultdict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
RATE_LIMIT_RPS = 1.0  # 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É
MAX_REQUESTS_TOTAL = 200
MAX_RETRIES = 3
BACKOFF_ON_403_429 = 60  # —Å–µ–∫—É–Ω–¥
HEADLESS_ENABLED = os.getenv("HEADLESS", "false").lower() == "true"

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Origin": "https://re.kufar.by",
    "Referer": "https://re.kufar.by/",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "ru-RU,ru;q=0.9",
}

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ candidate endpoints
LOCATION_KEYWORDS = ['autocomplete', 'locations', 'search', 'province', 'locality', 'place', 'region', 'city', 'location']

# –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è slug'–æ–≤
SLUG_PATTERN = re.compile(r'province-[a-z0-9_~-]+(?:~[^"\'<> \n]+)*')
# –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –≥–æ—Ä–æ–¥–æ–≤
RUSSIAN_LABEL_PATTERN = re.compile(r'[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*')


class RateLimiter:
    """Rate limiter –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    def __init__(self, rps: float = 1.0):
        self.rps = rps
        self.min_interval = 1.0 / rps
        self.last_request_time = 0.0
    
    async def wait(self):
        """–ñ–¥–µ—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        now = time.time()
        elapsed = now - self.last_request_time
        if elapsed < self.min_interval:
            await asyncio.sleep(self.min_interval - elapsed)
        self.last_request_time = time.time()


class InvestigationRunner:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è investigation"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.logs_dir = output_dir / "logs"
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        self.rate_limiter = RateLimiter(RATE_LIMIT_RPS)
        self.request_count = 0
        self.blocked_hosts: Set[str] = set()
        self.commands_run = []
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.har_analysis = []
        self.html_slug_samples = []
        self.header_tests = []
        self.city_map_candidates = []
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
        log_file = self.logs_dir / "kufar_investigate.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
    
    def log_command(self, cmd: str):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é –∫–æ–º–∞–Ω–¥—É"""
        self.commands_run.append(f"{datetime.now(timezone.utc).isoformat()}: {cmd}")
        logger.info(f"Command: {cmd}")
    
    async def find_artifacts(self, repo_root: Path) -> Dict[str, List[Path]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏"""
        logger.info("Step 1: –ü–æ–∏—Å–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏...")
        
        artifacts = {
            'har': [],
            'html': [],
            'json': [],
            'log': [],
        }
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        extensions = {
            'har': ['.har'],
            'html': ['.html', '.htm'],
            'json': ['.json'],
            'log': ['.log'],
        }
        
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        ignore_dirs = {'.git', '__pycache__', 'node_modules', '.venv', 'venv', 'env'}
        
        for root, dirs, files in os.walk(repo_root):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [d for d in dirs if d not in ignore_dirs]
            
            for file in files:
                file_path = Path(root) / file
                rel_path = file_path.relative_to(repo_root)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                ext = file_path.suffix.lower()
                for artifact_type, exts in extensions.items():
                    if ext in exts:
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è JSON/LOG
                        if artifact_type in ['json', 'log']:
                            try:
                                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                    content = f.read(1000).lower()
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                                    if any(kw in content for kw in ['kufar', 'location', 'autocomplete', 'province', 'locality']):
                                        artifacts[artifact_type].append(file_path)
                            except:
                                pass
                        else:
                            artifacts[artifact_type].append(file_path)
                        break
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: HAR={len(artifacts['har'])}, HTML={len(artifacts['html'])}, JSON={len(artifacts['json'])}, LOG={len(artifacts['log'])}")
        return artifacts
    
    async def analyze_har_logs(self, artifacts: Dict[str, List[Path]]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç HAR —Ñ–∞–π–ª—ã –∏ –ª–æ–≥–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ candidate endpoints"""
        logger.info("Step 2: –ê–Ω–∞–ª–∏–∑ HAR/–ª–æ–≥–æ–≤...")
        
        all_files = artifacts['har'] + artifacts['json'] + artifacts['log']
        
        for file_path in all_files:
            try:
                logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path.name}")
                
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON (HAR –∏–ª–∏ JSON –ª–æ–≥)
                try:
                    data = json.loads(content)
                    await self._parse_har_json(data, str(file_path))
                except json.JSONDecodeError:
                    # –ü—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ª–æ–≥
                    await self._parse_text_log(content, str(file_path))
            
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {file_path}: {e}")
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(self.har_analysis)} candidate endpoints")
    
    async def _parse_har_json(self, data: Any, source_file: str):
        """–ü–∞—Ä—Å–∏—Ç HAR –∏–ª–∏ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
        if isinstance(data, dict):
            # HAR —Ñ–æ—Ä–º–∞—Ç
            if 'log' in data:
                entries = data['log'].get('entries', [])
                for entry in entries:
                    await self._process_har_entry(entry, source_file)
            # –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –º–∞—Å—Å–∏–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
            elif 'entries' in data:
                for entry in data['entries']:
                    await self._process_har_entry(entry, source_file)
            # –ò–ª–∏ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–≤–µ—Ç API –Ω–∞–ø—Ä—è–º—É—é
            elif any(kw in str(data).lower() for kw in LOCATION_KEYWORDS):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                self.har_analysis.append({
                    'source_file': source_file,
                    'type': 'api_response',
                    'data': data,
                })
        
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    await self._parse_har_json(item, source_file)
    
    async def _process_har_entry(self, entry: Dict, source_file: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å –∏–∑ HAR"""
        request = entry.get('request', {})
        response = entry.get('response', {})
        
        url = request.get('url', '')
        method = request.get('method', 'GET')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ URL –∏–ª–∏ body
        url_lower = url.lower()
        body_text = ''
        
        if 'postData' in request:
            body_text = request['postData'].get('text', '').lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º response body
        if 'content' in response:
            response_text = response['content'].get('text', '').lower()
            body_text += ' ' + response_text
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(kw in url_lower or kw in body_text for kw in LOCATION_KEYWORDS):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            headers = {}
            for header in request.get('headers', []):
                name = header.get('name', '')
                value = header.get('value', '')
                if name.lower() not in ['cookie', 'authorization']:  # –ù–µ –ª–æ–≥–∏—Ä—É–µ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    headers[name] = value
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º body
            request_body = None
            if 'postData' in request:
                request_body = request['postData'].get('text', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º response
            response_status = response.get('status', 0)
            response_body = ''
            if 'content' in response:
                response_body = response['content'].get('text', '')[:4000]
            
            self.har_analysis.append({
                'source_file': source_file,
                'method': method,
                'url': url,
                'request_headers': headers,
                'request_body': request_body,
                'response_status': response_status,
                'response_body': response_body[:4000] if response_body else None,
            })
    
    async def _parse_text_log(self, content: str, source_file: str):
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ª–æ–≥ –¥–ª—è –ø–æ–∏—Å–∫–∞ URL'–æ–≤"""
        # –ò—â–µ–º URL'—ã —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        url_pattern = re.compile(r'https?://[^\s<>"\'\)]+')
        urls = url_pattern.findall(content)
        
        for url in urls:
            url_lower = url.lower()
            if any(kw in url_lower for kw in LOCATION_KEYWORDS):
                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –º–µ—Ç–æ–¥ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                method = 'GET'
                if 'POST' in content[max(0, content.find(url) - 50):content.find(url)]:
                    method = 'POST'
                
                self.har_analysis.append({
                    'source_file': source_file,
                    'method': method,
                    'url': url,
                    'request_headers': {},
                    'request_body': None,
                    'response_status': 0,
                    'response_body': None,
                })
    
    async def extract_html_slugs(self, artifacts: Dict[str, List[Path]]):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç slug'–∏ –∏–∑ HTML —Ñ–∞–π–ª–æ–≤"""
        logger.info("Step 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ slug'–æ–≤ –∏–∑ HTML...")
        
        for html_file in artifacts['html']:
            try:
                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ HTML: {html_file.name}")
                
                with open(html_file, 'r', encoding='utf-8', errors='ignore') as f:
                    html_content = f.read()
                
                await self._parse_html_content(html_content, str(html_file))
            
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML {html_file}: {e}")
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(self.html_slug_samples)} slug'–æ–≤ –≤ HTML")
    
    async def _parse_html_content(self, html: str, source_file: str):
        """–ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ slug'–æ–≤ –∏ labels"""
        # –ò—â–µ–º JSON –≤ script —Ç–µ–≥–∞—Ö
        script_pattern = re.compile(r'<script[^>]*id="__NEXT_DATA__"[^>]*>(.*?)</script>', re.DOTALL | re.IGNORECASE)
        script_matches = script_pattern.findall(html)
        
        for script_content in script_matches:
            try:
                data = json.loads(script_content)
                await self._extract_from_json_data(data, source_file, html)
            except json.JSONDecodeError:
                pass
        
        # –ò—â–µ–º –±–æ–ª—å—à–∏–µ JSON –±–ª–æ–∫–∏
        json_pattern = re.compile(r'\{[^{}]*"province"[^{}]*\}', re.IGNORECASE | re.DOTALL)
        json_matches = json_pattern.findall(html)
        
        for json_str in json_matches:
            try:
                data = json.loads(json_str)
                await self._extract_from_json_data(data, source_file, html)
            except json.JSONDecodeError:
                pass
        
        # –ò—â–µ–º slug'–∏ –Ω–∞–ø—Ä—è–º—É—é –≤ HTML
        slug_matches = SLUG_PATTERN.findall(html)
        for slug in slug_matches:
            await self._extract_slug_with_context(slug, html, source_file)
    
    async def _extract_from_json_data(self, data: Any, source_file: str, html: str):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        if isinstance(data, dict):
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º slug'–∏ –∏ labels
            for key, value in data.items():
                if isinstance(value, (dict, list)):
                    await self._extract_from_json_data(value, source_file, html)
                elif isinstance(value, str):
                    if SLUG_PATTERN.search(value):
                        await self._extract_slug_with_context(value, html, source_file)
                    elif RUSSIAN_LABEL_PATTERN.search(value):
                        # –ú–æ–∂–µ—Ç –±—ã—Ç—å label –¥–ª—è slug'–∞
                        pass
        
        elif isinstance(data, list):
            for item in data:
                await self._extract_from_json_data(item, source_file, html)
    
    async def _extract_slug_with_context(self, slug: str, html: str, source_file: str):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç slug —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (label)"""
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é slug'–∞ –≤ HTML
        slug_pos = html.find(slug)
        if slug_pos == -1:
            return
        
        # –ë–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç ¬±500 —Å–∏–º–≤–æ–ª–æ–≤
        context_start = max(0, slug_pos - 500)
        context_end = min(len(html), slug_pos + len(slug) + 500)
        context = html[context_start:context_end]
        
        # –ò—â–µ–º —Ä—É—Å—Å–∫–∏–π label —Ä—è–¥–æ–º
        label_match = RUSSIAN_LABEL_PATTERN.search(context)
        label_ru = label_match.group(0) if label_match else None
        
        # –ò—â–µ–º –±–µ–ª–æ—Ä—É—Å—Å–∫–∏–π label (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ)
        label_by = None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º snippet
        snippet = context[:200] + '...' if len(context) > 200 else context
        
        self.html_slug_samples.append({
            'slug': slug,
            'label_ru': label_ru,
            'label_by': label_by,
            'source_file': source_file,
            'context_snippet': snippet,
        })
    
    async def test_endpoints(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ candidate endpoints"""
        logger.info("Step 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ endpoints...")
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ endpoints
        endpoints = {}
        for entry in self.har_analysis:
            url = entry.get('url', '')
            if not url:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL (—É–±–∏—Ä–∞–µ–º query params –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏)
            parsed = urlparse(url)
            base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            
            if base_url not in endpoints:
                endpoints[base_url] = {
                    'url': url,
                    'method': entry.get('method', 'GET'),
                    'headers': entry.get('request_headers', {}),
                    'params': parse_qs(parsed.query),
                }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ endpoints –∏–∑ –∫–æ–¥–∞
        known_endpoints = [
            {
                'url': 'https://api.kufar.by/search-api/v1/autocomplete/location',
                'method': 'GET',
                'headers': {},
                'params': {'q': '–ü–æ–ª–æ—Ü–∫'},
            },
            {
                'url': 'https://www.kufar.by/api/search/locations',
                'method': 'GET',
                'headers': {},
                'params': {'query': '–ü–æ–ª–æ—Ü–∫'},
            },
        ]
        
        for ep in known_endpoints:
            base_url = urlparse(ep['url']).path
            if base_url not in endpoints:
                endpoints[base_url] = ep
        
        logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(endpoints)} endpoints...")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π endpoint
        for base_url, ep_config in list(endpoints.items())[:50]:  # –õ–∏–º–∏—Ç 50 endpoints
            if self.request_count >= MAX_REQUESTS_TOTAL:
                logger.warning("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
                break
            
            host = urlparse(ep_config['url']).netloc
            if host in self.blocked_hosts:
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ö–æ—Å—Ç: {host}")
                continue
            
            await self._test_single_endpoint(ep_config)
            await self.rate_limiter.wait()
    
    async def _test_single_endpoint(self, ep_config: Dict):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω endpoint"""
        url = ep_config['url']
        method = ep_config['method']
        headers = {**DEFAULT_HEADERS, **ep_config.get('headers', {})}
        params = ep_config.get('params', {})
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å query params –≤ URL, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö
        parsed = urlparse(url)
        if parsed.query:
            existing_params = parse_qs(parsed.query)
            params = {**existing_params, **params}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π URL
        if params and method == 'GET':
            query_string = urlencode(params, doseq=True)
            final_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query_string}"
        else:
            final_url = url
        
        logger.info(f"–¢–µ—Å—Ç: {method} {final_url}")
        
        start_time = time.time()
        result = {
            'url': final_url,
            'method': method,
            'request_headers': headers,
            'status': None,
            'response_headers': {},
            'body': None,
            'latency_ms': 0,
            'error': None,
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                for attempt in range(MAX_RETRIES):
                    try:
                        if method == 'GET':
                            async with session.get(final_url, headers=headers) as response:
                                result['status'] = response.status
                                result['response_headers'] = dict(response.headers)
                                body_text = await response.text()
                                result['body'] = body_text[:5000]
                                
                                # –ü–∞—Ä—Å–∏–º JSON –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
                                if response.status == 200:
                                    try:
                                        json_data = await response.json()
                                        await self._extract_city_data_from_response(json_data, final_url)
                                    except:
                                        pass
                                
                                break
                        else:
                            # POST –∑–∞–ø—Ä–æ—Å
                            body = ep_config.get('request_body')
                            async with session.post(final_url, headers=headers, data=body) as response:
                                result['status'] = response.status
                                result['response_headers'] = dict(response.headers)
                                body_text = await response.text()
                                result['body'] = body_text[:5000]
                                
                                if response.status == 200:
                                    try:
                                        json_data = await response.json()
                                        await self._extract_city_data_from_response(json_data, final_url)
                                    except:
                                        pass
                                
                                break
                    
                    except asyncio.TimeoutError:
                        if attempt == MAX_RETRIES - 1:
                            result['error'] = 'timeout'
                        else:
                            await asyncio.sleep(2 ** attempt)
                    
                    except Exception as e:
                        if attempt == MAX_RETRIES - 1:
                            result['error'] = str(e)
                        else:
                            await asyncio.sleep(2 ** attempt)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if result['status'] in [403, 429]:
                host = urlparse(final_url).netloc
                self.blocked_hosts.add(host)
                logger.warning(f"–•–æ—Å—Ç {host} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω (status {result['status']}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞–ª—å–Ω–µ–π—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã")
                await asyncio.sleep(BACKOFF_ON_403_429)
            
            result['latency_ms'] = int((time.time() - start_time) * 1000)
            self.request_count += 1
        
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ {final_url}: {e}")
        
        self.header_tests.append(result)
    
    async def _extract_city_data_from_response(self, data: Any, source_url: str):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –≥–æ—Ä–æ–¥–∞—Ö –∏–∑ JSON –æ—Ç–≤–µ—Ç–∞"""
        if isinstance(data, dict):
            # –ò—â–µ–º –ø–æ–ª—è —Å –≥–æ—Ä–æ–¥–∞–º–∏
            city_fields = ['city', 'province', 'locality', 'value', 'slug', 'id', 'name', 'locations', 'data']
            
            for key, value in data.items():
                if key.lower() in city_fields:
                    if isinstance(value, list):
                        for item in value:
                            await self._process_city_item(item, source_url)
                    elif isinstance(value, dict):
                        await self._process_city_item(value, source_url)
                else:
                    await self._extract_city_data_from_response(value, source_url)
        
        elif isinstance(data, list):
            for item in data:
                await self._extract_city_data_from_response(item, source_url)
    
    async def _process_city_item(self, item: Dict, source_url: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –≥–æ—Ä–æ–¥–∞"""
        if not isinstance(item, dict):
            return
        
        slug = item.get('slug', '')
        name = item.get('name', '') or item.get('value', '')
        
        if slug or name:
            self.city_map_candidates.append({
                'slug': slug,
                'label_ru': name,
                'label_by': item.get('name_by', ''),
                'sample_location_string': item.get('location', ''),
                'sample_coords': {
                    'lat': item.get('lat'),
                    'lng': item.get('lng'),
                } if item.get('lat') and item.get('lng') else None,
                'source': source_url,
            })
    
    async def test_html_extract_live(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ HTML –≤–∂–∏–≤—É—é"""
        logger.info("Step 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-extract –≤–∂–∏–≤—É—é...")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ slug'–∏
        test_slugs = [
            'country-belarus~province-minsk~locality-minsk',
            'country-belarus~province-brestskaja_oblast~locality-baranovichi',
            'country-belarus~province-vitebskaja_oblast~locality-polotsk',
            'country-belarus~province-vitebskaja_oblast~locality-orsha',
        ]
        
        test_urls = [
            'https://re.kufar.by/',
            'https://www.kufar.by/',
        ]
        
        for url in test_urls:
            if self.request_count >= MAX_REQUESTS_TOTAL:
                break
            
            host = urlparse(url).netloc
            if host in self.blocked_hosts:
                continue
            
            await self.rate_limiter.wait()
            
            try:
                timeout = aiohttp.ClientTimeout(total=10)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url, headers=DEFAULT_HEADERS) as response:
                        if response.status == 200:
                            html = await response.text()
                            await self._parse_html_content(html, f"live_{url}")
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ header_tests
                            self.header_tests.append({
                                'url': url,
                                'method': 'GET',
                                'status': response.status,
                                'body': html[:5000],
                                'latency_ms': 0,
                            })
                            
                            self.request_count += 1
            
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
    
    async def run_headless_probe(self):
        """–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π headless probe —á–µ—Ä–µ–∑ Playwright"""
        if not HEADLESS_ENABLED:
            logger.info("Step 6: Headless probe –ø—Ä–æ–ø—É—â–µ–Ω (HEADLESS=false)")
            return
        
        logger.info("Step 6: –ó–∞–ø—É—Å–∫ headless probe...")
        
        try:
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    user_agent=DEFAULT_HEADERS['User-Agent'],
                    viewport={'width': 1920, 'height': 1080},
                )
                page = await context.new_page()
                
                # –ü–æ–¥–ø–∏—Å—ã–≤–∞–µ–º—Å—è –Ω–∞ XHR –∑–∞–ø—Ä–æ—Å—ã
                xhr_responses = []
                
                async def handle_response(response):
                    url = response.url
                    if any(kw in url.lower() for kw in LOCATION_KEYWORDS):
                        try:
                            body = await response.text()
                            xhr_responses.append({
                                'url': url,
                                'status': response.status,
                                'headers': dict(response.headers),
                                'body': body[:5000],
                            })
                        except:
                            pass
                
                page.on('response', handle_response)
                
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                await page.goto('https://re.kufar.by/', wait_until='networkidle')
                await asyncio.sleep(2)
                
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞ –∏ –≤–≤–µ—Å—Ç–∏ "–ü–æ–ª–æ—Ü–∫"
                try:
                    # –ò—â–µ–º –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞
                    search_input = await page.query_selector('input[type="search"], input[placeholder*="–≥–æ—Ä–æ–¥"], input[name*="search"]')
                    if search_input:
                        await search_input.fill('–ü–æ–ª–æ—Ü–∫')
                        await asyncio.sleep(1)
                        
                        # –ñ–¥–µ–º –æ—Ç–≤–µ—Ç–æ–≤
                        await asyncio.sleep(3)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞: {e}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HAR
                har = await context.storage_state()
                
                await browser.close()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ XHR –æ—Ç–≤–µ—Ç—ã
                for xhr in xhr_responses:
                    self.header_tests.append({
                        'url': xhr['url'],
                        'method': 'GET',
                        'status': xhr['status'],
                        'response_headers': xhr['headers'],
                        'body': xhr['body'],
                        'source': 'headless',
                    })
                    
                    # –ü–∞—Ä—Å–∏–º JSON –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
                    try:
                        json_data = json.loads(xhr['body'])
                        await self._extract_city_data_from_response(json_data, xhr['url'])
                    except:
                        pass
        
        except ImportError:
            logger.warning("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º headless probe")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ headless probe: {e}")
    
    def aggregate_city_map(self):
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç city_map_candidates"""
        logger.info("Step 7: –ê–≥—Ä–µ–≥–∞—Ü–∏—è city_map...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        slug_map = {}
        
        # –ò–∑ HTML samples
        for sample in self.html_slug_samples:
            slug = sample['slug']
            if slug not in slug_map:
                slug_map[slug] = {
                    'slug': slug,
                    'label_ru': sample.get('label_ru'),
                    'label_by': sample.get('label_by'),
                    'sample_location_string': None,
                    'sample_coords': None,
                    'sources': [sample['source_file']],
                }
        
        # –ò–∑ API responses
        for candidate in self.city_map_candidates:
            slug = candidate.get('slug', '')
            if slug:
                if slug not in slug_map:
                    slug_map[slug] = {
                        'slug': slug,
                        'label_ru': candidate.get('label_ru'),
                        'label_by': candidate.get('label_by'),
                        'sample_location_string': candidate.get('sample_location_string'),
                        'sample_coords': candidate.get('sample_coords'),
                        'sources': [],
                    }
                slug_map[slug]['sources'].append(candidate.get('source', 'unknown'))
        
        self.city_map_candidates = list(slug_map.values())
        logger.info(f"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ {len(self.city_map_candidates)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö slug'–æ–≤")
    
    def generate_decision_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç decision report"""
        logger.info("Step 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è decision report...")
        
        report_lines = []
        report_lines.append("# Kufar City Lookup Investigation Report\n")
        report_lines.append(f"Generated: {datetime.now(timezone.utc).isoformat()}\n\n")
        
        # –ê–Ω–∞–ª–∏–∑ endpoints
        working_endpoints = [t for t in self.header_tests if t.get('status') == 200]
        json_endpoints = []
        location_endpoints = []
        
        for test in working_endpoints:
            body = test.get('body', '')
            url = test.get('url', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ JSON
            try:
                json_data = json.loads(body)
                json_endpoints.append(test)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ª–æ–∫–∞—Ü–∏—è—Ö
                body_lower = str(json_data).lower()
                if any(kw in body_lower for kw in ['location', 'city', 'province', 'locality', 'slug']):
                    location_endpoints.append(test)
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ endpoints –Ω–∞ 404
        known_endpoints_404 = [
            t for t in self.header_tests 
            if t.get('status') == 404 and any(ep in t.get('url', '') for ep in ['autocomplete', 'locations'])
        ]
        
        report_lines.append("## Endpoint Analysis\n\n")
        
        if location_endpoints:
            report_lines.append("### ‚úÖ USE_ENDPOINT_WITH_HEADERS (RECOMMENDED)\n\n")
            report_lines.append(f"**Confidence:** HIGH\n")
            report_lines.append(f"**Estimated Effort:** L (Low)\n\n")
            report_lines.append(f"–ù–∞–π–¥–µ–Ω–æ {len(location_endpoints)} —Ä–∞–±–æ—á–∏—Ö endpoint'–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ª–æ–∫–∞—Ü–∏—è—Ö:\n\n")
            
            for ep in location_endpoints[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                report_lines.append(f"- **URL:** `{ep['url'][:100]}...`\n")
                report_lines.append(f"  - Method: {ep.get('method', 'GET')}\n")
                report_lines.append(f"  - Status: {ep.get('status')}\n")
                report_lines.append(f"  - Latency: {ep.get('latency_ms', 0)}ms\n")
                report_lines.append(f"  - Headers required: {list(ep.get('request_headers', {}).keys())[:5]}\n\n")
            
            report_lines.append("**Next Steps:**\n")
            report_lines.append("1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–π endpoint —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏\n")
            report_lines.append("2. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ `scrapers/kufar.py::lookup_kufar_location_async`\n")
            report_lines.append("3. –û–±–Ω–æ–≤–∏—Ç—å `KUFAR_SUGGEST_URLS` –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n\n")
        elif known_endpoints_404:
            report_lines.append("### ‚ùå KNOWN_ENDPOINTS_NOT_WORKING\n\n")
            report_lines.append(f"**Confidence:** HIGH\n")
            report_lines.append(f"**Status:** –ò–∑–≤–µ—Å—Ç–Ω—ã–µ endpoints –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç 404\n\n")
            report_lines.append("–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ endpoints:\n\n")
            for ep in known_endpoints_404:
                report_lines.append(f"- `{ep['url']}` ‚Üí Status: {ep.get('status')}\n")
            report_lines.append("\n**–í—ã–≤–æ–¥:** API endpoints –¥–ª—è autocomplete/locations –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å.\n\n")
        
        # HTML extract –∞–Ω–∞–ª–∏–∑
        if self.html_slug_samples or self.city_map_candidates:
            report_lines.append("### ‚úÖ USE_HTML_EXTRACT (RECOMMENDED)\n\n")
            confidence = 'HIGH' if len(self.city_map_candidates) > 50 else 'MEDIUM'
            report_lines.append(f"**Confidence:** {confidence}\n")
            report_lines.append(f"**Estimated Effort:** M (Medium)\n\n")
            report_lines.append(f"–ù–∞–π–¥–µ–Ω–æ {len(self.html_slug_samples)} slug'–æ–≤ –≤ HTML –∏ {len(self.city_map_candidates)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤.\n\n")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
            report_lines.append("–ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö slug'–æ–≤:\n\n")
            for sample in self.city_map_candidates[:5]:
                slug = sample.get('slug', '')
                label = sample.get('label_ru', 'N/A')
                report_lines.append(f"- `{slug}` ‚Üí {label}\n")
            report_lines.append("\n")
            
            report_lines.append("**Next Steps:**\n")
            report_lines.append("1. –°–æ–±—Ä–∞—Ç—å –ø–æ–ª–Ω—ã–π city_map –∏–∑ HTML (one-time run)\n")
            report_lines.append("2. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ç–∞–±–ª–∏—Ü—É `city_codes` –∏–ª–∏ —Ñ–∞–π–ª `data/kufar_city_map.json`\n")
            report_lines.append("3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ lookup (endpoints –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç)\n")
            report_lines.append("4. –û–±–Ω–æ–≤–∏—Ç—å `_get_city_gtsy()` –≤ `scrapers/kufar.py` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è city_map\n\n")
            
            report_lines.append("**Integration Example:**\n")
            report_lines.append("```python\n")
            report_lines.append("# –í scrapers/kufar.py\n")
            report_lines.append("def _get_city_gtsy(self, city: str | dict) -> str:\n")
            report_lines.append("    # –ó–∞–≥—Ä—É–∑–∏—Ç—å city_map –∏–∑ JSON –∏–ª–∏ –ë–î\n")
            report_lines.append("    city_map = load_city_map()  # –∏–∑ data/kufar_city_map.json\n")
            report_lines.append("    city_name = city if isinstance(city, str) else city.get('name', '')\n")
            report_lines.append("    city_lower = city_name.lower().strip()\n")
            report_lines.append("    return city_map.get(city_lower, city_map['–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏'])  # fallback\n")
            report_lines.append("```\n\n")
        
        # Headless –∞–Ω–∞–ª–∏–∑
        if HEADLESS_ENABLED:
            report_lines.append("### üîÑ USE_HEADLESS (OPTIONAL)\n\n")
            report_lines.append(f"**Confidence:** LOW\n")
            report_lines.append(f"**Estimated Effort:** H (High)\n\n")
            report_lines.append("Headless –±—Ä–∞—É–∑–µ—Ä –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n")
            report_lines.append("**Next Steps:**\n")
            report_lines.append("1. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Playwright-based crawler\n")
            report_lines.append("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è one-time crawl\n")
            report_lines.append("3. –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        report_lines.append("## Final Recommendation\n\n")
        
        if location_endpoints:
            report_lines.append("**‚úÖ USE_ENDPOINT_WITH_HEADERS**\n\n")
            report_lines.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ä–∞–±–æ—á–∏–π endpoint —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.\n")
        elif self.city_map_candidates:
            report_lines.append("**‚úÖ USE_HTML_EXTRACT** (PRIMARY METHOD)\n\n")
            report_lines.append("–°–æ–±—Ä–∞—Ç—å city_map –∏–∑ HTML –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ lookup.\n")
            report_lines.append(f"–ù–∞–π–¥–µ–Ω–æ {len(self.city_map_candidates)} –≥–æ—Ä–æ–¥–æ–≤, —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã.\n")
        else:
            report_lines.append("**üîÑ USE_HEADLESS**\n\n")
            report_lines.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å headless –±—Ä–∞—É–∑–µ—Ä –¥–ª—è one-time crawl.\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report_lines.append("\n## Statistics\n\n")
        report_lines.append(f"- HAR/Log candidates: {len(self.har_analysis)}\n")
        report_lines.append(f"- HTML slug samples: {len(self.html_slug_samples)}\n")
        report_lines.append(f"- Header tests performed: {len(self.header_tests)}\n")
        report_lines.append(f"- City map candidates: {len(self.city_map_candidates)}\n")
        report_lines.append(f"- Requests made: {self.request_count}\n")
        report_lines.append(f"- Blocked hosts: {len(self.blocked_hosts)}\n")
        
        return '\n'.join(report_lines)
    
    def save_artifacts(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã"""
        logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤...")
        
        # har_analysis.json
        with open(self.output_dir / 'har_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(self.har_analysis, f, indent=2, ensure_ascii=False)
        
        # html_slug_samples.json
        with open(self.output_dir / 'html_slug_samples.json', 'w', encoding='utf-8') as f:
            json.dump(self.html_slug_samples, f, indent=2, ensure_ascii=False)
        
        # header_tests.json
        with open(self.output_dir / 'header_tests.json', 'w', encoding='utf-8') as f:
            json.dump(self.header_tests, f, indent=2, ensure_ascii=False)
        
        # city_map_candidates.json
        with open(self.output_dir / 'city_map_candidates.json', 'w', encoding='utf-8') as f:
            json.dump(self.city_map_candidates, f, indent=2, ensure_ascii=False)
        
        # decision_report.md
        report = self.generate_decision_report()
        with open(self.output_dir / 'decision_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        # commands_run.txt
        with open(self.output_dir / 'commands_run.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(self.commands_run))
        
        logger.info("–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def create_zip(self) -> Path:
        """–°–æ–∑–¥–∞–µ—Ç ZIP –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ ZIP –∞—Ä—Ö–∏–≤–∞...")
        
        zip_path = Path(str(self.output_dir) + '.zip')
        
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(self.output_dir):
                for file in files:
                    file_path = Path(root) / file
                    arcname = file_path.relative_to(self.output_dir.parent)
                    zipf.write(file_path, arcname)
        
        logger.info(f"ZIP –∞—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: {zip_path}")
        return zip_path
    
    async def run(self, repo_root: Path):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ investigation"""
        logger.info("=" * 80)
        logger.info("Kufar City Lookup Investigation")
        logger.info("=" * 80)
        
        self.log_command("python tools/kufar_investigate.py")
        
        # Step 1: –ü–æ–∏—Å–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        artifacts = await self.find_artifacts(repo_root)
        
        # Step 2: –ê–Ω–∞–ª–∏–∑ HAR/–ª–æ–≥–æ–≤
        await self.analyze_har_logs(artifacts)
        
        # Step 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ slug'–æ–≤ –∏–∑ HTML
        await self.extract_html_slugs(artifacts)
        
        # Step 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ endpoints
        await self.test_endpoints()
        
        # Step 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-extract –≤–∂–∏–≤—É—é
        await self.test_html_extract_live()
        
        # Step 6: Headless probe (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        await self.run_headless_probe()
        
        # Step 7: –ê–≥—Ä–µ–≥–∞—Ü–∏—è
        self.aggregate_city_map()
        
        # Step 8: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        self.save_artifacts()
        
        # Step 9: –°–æ–∑–¥–∞–Ω–∏–µ ZIP
        zip_path = self.create_zip()
        
        logger.info("=" * 80)
        logger.info("Investigation –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {self.output_dir}")
        logger.info(f"ZIP –∞—Ä—Ö–∏–≤: {zip_path}")
        logger.info("=" * 80)
        
        return zip_path


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent
    
    # –°–æ–∑–¥–∞–µ–º output –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
    output_dir = Path('/tmp') / f'kufar_investigation_{timestamp}'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º investigation
    runner = InvestigationRunner(output_dir)
    zip_path = await runner.run(repo_root)
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print("\n" + "=" * 80)
    print("INVESTIGATION COMPLETE")
    print("=" * 80)
    print(f"\nResults directory: {output_dir}")
    print(f"ZIP archive: {zip_path}")
    print("\nKey files:")
    print(f"  - har_analysis.json: {len(runner.har_analysis)} candidates")
    print(f"  - html_slug_samples.json: {len(runner.html_slug_samples)} slugs")
    print(f"  - header_tests.json: {len(runner.header_tests)} tests")
    print(f"  - city_map_candidates.json: {len(runner.city_map_candidates)} cities")
    print(f"  - decision_report.md: See for recommendations")
    print("\nNext steps:")
    print("  1. Review decision_report.md")
    print("  2. Check header_tests.json for working endpoints")
    print("  3. Implement recommended approach")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
