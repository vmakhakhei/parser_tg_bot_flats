"""
–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ - —Å–æ–±–∏—Ä–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ö–∞–∂–¥—ã–π scraper –æ–±–µ—Ä–Ω—É—Ç –≤ try/except
- –ü—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ scraper –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
- –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ scraper
"""
import asyncio
import sys
import os
import time
import json
import aiohttp
from typing import List, Dict, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scrapers.base import Listing
from scrapers.kufar import KufarScraper
from scrapers.realt import RealtByScraper
from scrapers.domovita import DomovitaScraper
from scrapers.onliner import OnlinerRealtScraper
from scrapers.gohome import GoHomeScraper
from scrapers.etagi import EtagiScraper

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º error_logger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
try:
    from error_logger import log_error, log_warning, log_info
except ImportError:
    # Fallback –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    def log_error(source, message, exception=None):
        print(f"[ERROR] [{source}] {message}: {exception}")
    def log_warning(source, message):
        print(f"[WARN] [{source}] {message}")
    def log_info(source, message):
        print(f"[INFO] [{source}] {message}")

class ListingsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤"""
    
    # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
    SCRAPERS = {
        "kufar": KufarScraper,
        "realt": RealtByScraper,
        "domovita": DomovitaScraper,
        "onliner": OnlinerRealtScraper,
        "gohome": GoHomeScraper,
        "etagi": EtagiScraper,
    }
    
    def __init__(self, enabled_sources: Optional[List[str]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
        
        Args:
            enabled_sources: –°–ø–∏—Å–æ–∫ –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. 
                            –ï—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ.
        """
        if enabled_sources:
            self.enabled_sources = [s.lower() for s in enabled_sources]
        else:
            self.enabled_sources = list(self.SCRAPERS.keys())
    
    async def fetch_all_listings(
        self,
        city: str = "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms: int = 1,
        max_rooms: int = 4,
        min_price: int = 0,
        max_price: int = 100000,
    ) -> List[Listing]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        –ö–∞–∂–¥—ã–π scraper –æ–±–µ—Ä–Ω—É—Ç –≤ try/except, –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ
        –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        """
        all_listings = []
        tasks = []
        source_names = []
        
        log_info("aggregator", f"–ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å {len(self.enabled_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {', '.join(self.enabled_sources)}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        for source_name in self.enabled_sources:
            if source_name in self.SCRAPERS:
                try:
                    scraper_class = self.SCRAPERS[source_name]
                    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä scraper'–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                    try:
                        scraper_instance = scraper_class()
                    except Exception as e:
                        log_error("aggregator", f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ scraper '{source_name}'", e)
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–∞–¥–µ–Ω–∏–π
                    task = self._fetch_from_source(
                        scraper_instance,
                        source_name,
                        city, min_rooms, max_rooms, min_price, max_price
                    )
                    tasks.append(task)
                    source_names.append(source_name)
                except Exception as e:
                    log_error("aggregator", f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ scraper '{source_name}'", e)
                    continue
        
        if not tasks:
            log_warning("aggregator", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return []
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        # return_exceptions=True –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ scraper'–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç —Ä–∞–±–æ—Ç—É
        log_info("aggregator", f"–ó–∞–ø—É—Å–∫–∞—é {len(tasks)} –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        source_stats = {}
        successful_sources = 0
        failed_sources = 0
        
        for source_name, result in zip(source_names, results):
            if isinstance(result, Exception):
                # –û—à–∏–±–∫–∞ —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞ –≤ _fetch_from_source, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –∑–¥–µ—Å—å —Ç–æ–∂–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                log_error("aggregator", f"Scraper '{source_name}' –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {type(result).__name__}", result)
                source_stats[source_name] = {"error": str(result), "count": 0}
                failed_sources += 1
            elif result is None:
                # –ö–†–ò–¢–ò–ß–ù–û: None —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—à–∏–±–∫–æ–π
                log_error("aggregator", f"Scraper '{source_name}' –≤–µ—Ä–Ω—É–ª None - —ç—Ç–æ –æ—à–∏–±–∫–∞!")
                source_stats[source_name] = {"error": "–í–µ—Ä–Ω—É–ª None", "count": 0}
                failed_sources += 1
            elif isinstance(result, list):
                count = len(result)
                all_listings.extend(result)
                source_stats[source_name] = {"count": count, "error": None}
                successful_sources += 1
                log_info("aggregator", f"‚úÖ Scraper '{source_name}': –ø–æ–ª—É—á–µ–Ω–æ {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            else:
                log_error("aggregator", f"Scraper '{source_name}': –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {result}")
                source_stats[source_name] = {"error": f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}", "count": 0}
                failed_sources += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        log_info("aggregator", f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: —É—Å–ø–µ—à–Ω–æ {successful_sources}/{len(source_names)}, –æ—à–∏–±–æ–∫ {failed_sources}")
        log_info("aggregator", f"üìä –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(all_listings)}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        unique_listings = self._remove_duplicates(all_listings)
        duplicates_removed = len(all_listings) - len(unique_listings)
        if duplicates_removed > 0:
            log_info("aggregator", f"–£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –≤ —Ç–∞–±–ª–∏—Ü—É apartments
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –ë–î, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ –ø–∞–º—è—Ç–∏
        if unique_listings:
            try:
                from database_turso import sync_apartment_from_listing
                
                saved_count = 0
                for listing in unique_listings:
                    try:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –≤ apartments
                        success = await sync_apartment_from_listing(listing, raw_json="{}")
                        if success:
                            saved_count += 1
                            # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π –ª–æ–≥ –≤ aggregator
                            log_info("aggregator", f"[AGGREGATOR] persisted ad_id={listing.id} source={listing.source}")
                        else:
                            log_warning("aggregator", f"[AGGREGATOR] failed to persist ad_id={listing.id} source={listing.source}")
                    except Exception as e:
                        log_error("aggregator", f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è {listing.id} –≤ apartments", e)
                        continue
                
                log_info("aggregator", f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∏–∑ {len(unique_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —Ç–∞–±–ª–∏—Ü—É apartments")
            except ImportError as e:
                log_error("aggregator", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sync_apartment_from_listing: {e}")
            except Exception as e:
                log_error("aggregator", f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ apartments: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ) - —É –Ω–∞—Å –Ω–µ—Ç –¥–∞—Ç—ã, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ü–µ–Ω–µ
        unique_listings.sort(key=lambda x: x.price if x.price > 0 else 999999999)
        
        log_info("aggregator", f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(unique_listings)}")
        
        # –ö–†–ò–¢–ò–ß–ù–û: —è–≤–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
        if not isinstance(unique_listings, list):
            log_error("aggregator", f"–û–®–ò–ë–ö–ê: _remove_duplicates –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫: {type(unique_listings)}")
            return []
        
        return unique_listings
    
    async def _fetch_from_source(
        self,
        scraper,
        source_name: str,
        city: str,
        min_rooms: int,
        max_rooms: int,
        min_price: int,
        max_price: int,
    ) -> List[Listing]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        –û–±–µ—Ä–Ω—É—Ç –≤ try/except –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–∞–¥–µ–Ω–∏–π.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –æ—Å—Ç–∞–ª—å–Ω—ã–µ scraper'—ã –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç—É.
        
        Args:
            scraper: –≠–∫–∑–µ–º–ø–ª—è—Ä scraper'–∞
            source_name: –ò–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            city: –ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            min_rooms: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
            max_rooms: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
            min_price: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
            max_price: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        scraper_name = getattr(scraper, 'SOURCE_NAME', source_name)
        
        try:
            log_info("aggregator", f"üîÑ –ó–∞–ø—É—Å–∫–∞—é scraper '{scraper_name}' –¥–ª—è –≥–æ—Ä–æ–¥–∞ '{city}'...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scraper'–∞ (context manager)
            try:
                async with scraper:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                    listings = await scraper.fetch_listings(
                        city=city,
                        min_rooms=min_rooms,
                        max_rooms=max_rooms,
                        min_price=min_price,
                        max_price=max_price,
                    )
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –ö–†–ò–¢–ò–ß–ù–û: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–æ–∫, –Ω–µ None
                    if listings is None:
                        log_error("aggregator", f"Scraper '{scraper_name}' –≤–µ—Ä–Ω—É–ª None –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞ - —ç—Ç–æ –æ—à–∏–±–∫–∞!")
                        return []
                    
                    if not isinstance(listings, list):
                        log_error("aggregator", f"Scraper '{scraper_name}' –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫: {type(listings)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {listings}")
                        return []
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–µ—Ç–∞–ª—è–º–∏
                    count = len(listings)
                    log_info("aggregator", f"‚úÖ Scraper '{scraper_name}': –ø–æ–ª—É—á–µ–Ω–æ {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã - —ç—Ç–æ Listing –æ–±—ä–µ–∫—Ç—ã
                    if count > 0:
                        first_item = listings[0]
                        if not isinstance(first_item, Listing):
                            log_warning("aggregator", f"Scraper '{scraper_name}': –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –Ω–µ Listing, –∞ {type(first_item)}")
                    
                    return listings
                    
            except asyncio.TimeoutError as e:
                log_error("aggregator", f"Scraper '{scraper_name}': —Ç–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö", e)
                return []
            except aiohttp.ClientError as e:
                log_error("aggregator", f"Scraper '{scraper_name}': –æ—à–∏–±–∫–∞ HTTP-–∑–∞–ø—Ä–æ—Å–∞", e)
                return []
            except Exception as e:
                log_error("aggregator", f"Scraper '{scraper_name}': –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å context manager", e)
                return []
                
        except Exception as e:
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, –∏–º–ø–æ—Ä—Ç –∏ —Ç.–¥.)
            log_error("aggregator", f"Scraper '{scraper_name}': –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", e)
            return []
    
    def _remove_duplicates(self, listings: List[Listing]) -> List[Listing]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        seen_ids = set()
        unique = []
        
        for listing in listings:
            if listing.id not in seen_ids:
                seen_ids.add(listing.id)
                unique.append(listing)
        
        return unique
    
    @classmethod
    def get_available_sources(cls) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return list(cls.SCRAPERS.keys())


async def test_aggregator():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...\n")
    
    aggregator = ListingsAggregator()
    
    listings = await aggregator.fetch_all_listings(
        city="–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms=1,
        max_rooms=3,
        min_price=0,
        max_price=50000,
    )
    
    print(f"\n{'='*50}")
    print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    print(f"{'='*50}\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    for i, listing in enumerate(listings[:5], 1):
        print(f"--- –û–±—ä—è–≤–ª–µ–Ω–∏–µ {i} ---")
        print(f"üè∑Ô∏è  –ò—Å—Ç–æ—á–Ω–∏–∫: {listing.source}")
        print(f"üè† {listing.title}")
        print(f"üí∞ –¶–µ–Ω–∞: {listing.price_formatted}")
        print(f"üö™ –ö–æ–º–Ω–∞—Ç: {listing.rooms}")
        print(f"üìê –ü–ª–æ—â–∞–¥—å: {listing.area} –º¬≤")
        print(f"üìç –ê–¥—Ä–µ—Å: {listing.address}")
        print(f"üîó URL: {listing.url}")
        print(f"üì∏ –§–æ—Ç–æ: {len(listing.photos)} —à—Ç.")
        print()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    from collections import Counter
    sources = Counter(l.source for l in listings)
    for source, count in sources.most_common():
        print(f"  ‚Ä¢ {source}: {count}")


if __name__ == "__main__":
    asyncio.run(test_aggregator())

