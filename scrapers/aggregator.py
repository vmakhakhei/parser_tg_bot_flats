"""
–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ - —Å–æ–±–∏—Ä–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""
import asyncio
import sys
import os
import time
import json
from typing import List, Dict, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scrapers.base import Listing
from scrapers.kufar import KufarScraper
from scrapers.realt import RealtByScraper
from scrapers.domovita import DomovitaScraper
from scrapers.onliner import OnlinerRealtScraper
from scrapers.gohome import GoHomeScraper
from scrapers.etagi import EtagiScraper

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º error_logger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
try:
    from error_logger import log_error, log_warning, log_info
except ImportError:
    # Fallback –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    def log_error(source, message, exception=None):
        print(f"[ERROR] [{source}] {message}: {exception}")
    def log_warning(source, message):
        print(f"[WARN] [{source}] {message}")
    def log_info(source, message):
        print(f"[INFO] [{source}] {message}")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def _write_debug_log(data):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç debug –ª–æ–≥ –≤ —Ñ–∞–π–ª"""
    try:
        import os
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        log_path = os.path.join(base_dir, ".cursor", "debug.log")
        os.makedirs(os.path.dirname(log_path), exist_ok=True)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(data) + "\n")
    except Exception as e:
        try:
            log_error("aggregator", f"Debug log error: {e}")
        except:
            pass


class ListingsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤"""
    
    # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
    SCRAPERS = {
        "kufar": KufarScraper,
        "realt": RealtByScraper,
        "domovita": DomovitaScraper,
        "onliner": OnlinerRealtScraper,
        "gohome": GoHomeScraper,
        "etagi": EtagiScraper,
    }
    
    def __init__(self, enabled_sources: Optional[List[str]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
        
        Args:
            enabled_sources: –°–ø–∏—Å–æ–∫ –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. 
                            –ï—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ.
        """
        if enabled_sources:
            self.enabled_sources = [s.lower() for s in enabled_sources]
        else:
            self.enabled_sources = list(self.SCRAPERS.keys())
    
    async def fetch_all_listings(
        self,
        city: str = "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms: int = 1,
        max_rooms: int = 4,
        min_price: int = 0,
        max_price: int = 100000,
    ) -> List[Listing]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        """
        all_listings = []
        tasks = []
        source_names = []
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        for source_name in self.enabled_sources:
            if source_name in self.SCRAPERS:
                scraper_class = self.SCRAPERS[source_name]
                task = self._fetch_from_source(
                    scraper_class(),
                    city, min_rooms, max_rooms, min_price, max_price
                )
                tasks.append(task)
                source_names.append(source_name)
        
        # #region agent log
        _write_debug_log({
            "sessionId": "test-session",
            "runId": "run1",
            "hypothesisId": "C",
            "location": "aggregator.py:88",
            "message": "Aggregator fetch start",
            "data": {"city": city, "sources": source_names, "filters": {"min_rooms": min_rooms, "max_rooms": max_rooms, "min_price": min_price, "max_price": max_price}},
            "timestamp": int(time.time() * 1000)
        })
        # #endregion
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–∞—Ä—Å–µ—Ä—ã —Å–∞–º–∏ –ª–æ–≥–∏—Ä—É—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
        source_stats = {}
        for source_name, result in zip(source_names, results):
            if isinstance(result, Exception):
                log_error(source_name, f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", result)
                source_stats[source_name] = {"error": str(result), "count": 0}
            elif isinstance(result, list):
                all_listings.extend(result)
                source_stats[source_name] = {"count": len(result), "error": None}
        
        # #region agent log
        _write_debug_log({
            "sessionId": "test-session",
            "runId": "run1",
            "hypothesisId": "C",
            "location": "aggregator.py:105",
            "message": "Aggregator source results",
            "data": {"city": city, "source_stats": source_stats, "total_before_dedup": len(all_listings)},
            "timestamp": int(time.time() * 1000)
        })
        # #endregion
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        unique_listings = self._remove_duplicates(all_listings)
        
        # #region agent log
        _write_debug_log({
            "sessionId": "test-session",
            "runId": "run1",
            "hypothesisId": "C",
            "location": "aggregator.py:115",
            "message": "Aggregator deduplication",
            "data": {"city": city, "before_dedup": len(all_listings), "after_dedup": len(unique_listings), "duplicates_removed": len(all_listings) - len(unique_listings)},
            "timestamp": int(time.time() * 1000)
        })
        # #endregion
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ) - —É –Ω–∞—Å –Ω–µ—Ç –¥–∞—Ç—ã, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ü–µ–Ω–µ
        unique_listings.sort(key=lambda x: x.price if x.price > 0 else 999999999)
        
        return unique_listings
    
    async def _fetch_from_source(
        self,
        scraper,
        city: str,
        min_rooms: int,
        max_rooms: int,
        min_price: int,
        max_price: int,
    ) -> List[Listing]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            async with scraper:
                listings = await scraper.fetch_listings(
                    city=city,
                    min_rooms=min_rooms,
                    max_rooms=max_rooms,
                    min_price=min_price,
                    max_price=max_price,
                )
                return listings
        except Exception as e:
            log_error(scraper.SOURCE_NAME, f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö", e)
            return []
    
    def _remove_duplicates(self, listings: List[Listing]) -> List[Listing]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        seen_ids = set()
        unique = []
        
        for listing in listings:
            if listing.id not in seen_ids:
                seen_ids.add(listing.id)
                unique.append(listing)
        
        return unique
    
    @classmethod
    def get_available_sources(cls) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return list(cls.SCRAPERS.keys())


async def test_aggregator():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...\n")
    
    aggregator = ListingsAggregator()
    
    listings = await aggregator.fetch_all_listings(
        city="–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms=1,
        max_rooms=3,
        min_price=0,
        max_price=50000,
    )
    
    print(f"\n{'='*50}")
    print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    print(f"{'='*50}\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    for i, listing in enumerate(listings[:5], 1):
        print(f"--- –û–±—ä—è–≤–ª–µ–Ω–∏–µ {i} ---")
        print(f"üè∑Ô∏è  –ò—Å—Ç–æ—á–Ω–∏–∫: {listing.source}")
        print(f"üè† {listing.title}")
        print(f"üí∞ –¶–µ–Ω–∞: {listing.price_formatted}")
        print(f"üö™ –ö–æ–º–Ω–∞—Ç: {listing.rooms}")
        print(f"üìê –ü–ª–æ—â–∞–¥—å: {listing.area} –º¬≤")
        print(f"üìç –ê–¥—Ä–µ—Å: {listing.address}")
        print(f"üîó URL: {listing.url}")
        print(f"üì∏ –§–æ—Ç–æ: {len(listing.photos)} —à—Ç.")
        print()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    from collections import Counter
    sources = Counter(l.source for l in listings)
    for source, count in sources.most_common():
        print(f"  ‚Ä¢ {source}: {count}")


if __name__ == "__main__":
    asyncio.run(test_aggregator())

