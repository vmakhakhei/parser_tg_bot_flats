"""
–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ - —Å–æ–±–∏—Ä–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ö–∞–∂–¥—ã–π scraper –æ–±–µ—Ä–Ω—É—Ç –≤ try/except
- –ü—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ scraper –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
- –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ scraper
"""
import asyncio
import sys
import os
import time
import json
import aiohttp
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scrapers.base import Listing
from scrapers.kufar import KufarScraper
from scrapers.realt import RealtByScraper
from scrapers.domovita import DomovitaScraper
from scrapers.onliner import OnlinerRealtScraper
from scrapers.gohome import GoHomeScraper
from scrapers.etagi import EtagiScraper

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º error_logger –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
try:
    from error_logger import log_error, log_warning, log_info
except ImportError:
    # Fallback –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    def log_error(source, message, exception=None):
        print(f"[ERROR] [{source}] {message}: {exception}")
    def log_warning(source, message):
        print(f"[WARN] [{source}] {message}")
    def log_info(source, message):
        print(f"[INFO] [{source}] {message}")

class ListingsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤"""
    
    # –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
    SCRAPERS = {
        "kufar": KufarScraper,
        "realt": RealtByScraper,
        "domovita": DomovitaScraper,
        "onliner": OnlinerRealtScraper,
        "gohome": GoHomeScraper,
        "etagi": EtagiScraper,
    }
    
    def __init__(self, enabled_sources: Optional[List[str]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞
        
        Args:
            enabled_sources: –°–ø–∏—Å–æ–∫ –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. 
                            –ï—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å–µ.
        """
        if enabled_sources:
            self.enabled_sources = [s.lower() for s in enabled_sources]
        else:
            self.enabled_sources = list(self.SCRAPERS.keys())
    
    async def fetch_all_listings(
        self,
        city: str = "–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms: int = 1,
        max_rooms: int = 4,
        min_price: int = 0,
        max_price: int = 100000,
    ) -> tuple[List[Listing], List[Dict[str, Any]]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å–æ –≤—Å–µ—Ö –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        –ö–∞–∂–¥—ã–π scraper –æ–±–µ—Ä–Ω—É—Ç –≤ try/except, –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ
        –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        """
        all_listings = []
        tasks = []
        source_names = []
        
        log_info("aggregator", f"–ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å {len(self.enabled_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {', '.join(self.enabled_sources)}")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        for source_name in self.enabled_sources:
            if source_name in self.SCRAPERS:
                try:
                    scraper_class = self.SCRAPERS[source_name]
                    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä scraper'–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                    try:
                        scraper_instance = scraper_class()
                    except Exception as e:
                        log_error("aggregator", f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ scraper '{source_name}'", e)
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–∞–¥–µ–Ω–∏–π
                    task = self._fetch_from_source(
                        scraper_instance,
                        source_name,
                        city, min_rooms, max_rooms, min_price, max_price
                    )
                    tasks.append(task)
                    source_names.append(source_name)
                except Exception as e:
                    log_error("aggregator", f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ scraper '{source_name}'", e)
                    continue
        
        if not tasks:
            log_warning("aggregator", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return []
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        # return_exceptions=True –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ scraper'–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç —Ä–∞–±–æ—Ç—É
        log_info("aggregator", f"–ó–∞–ø—É—Å–∫–∞—é {len(tasks)} –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        source_stats = {}
        successful_sources = 0
        failed_sources = 0
        
        for source_name, result in zip(source_names, results):
            if isinstance(result, Exception):
                # –û—à–∏–±–∫–∞ —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∞ –≤ _fetch_from_source, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –∑–¥–µ—Å—å —Ç–æ–∂–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                log_error("aggregator", f"Scraper '{source_name}' –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {type(result).__name__}", result)
                source_stats[source_name] = {"error": str(result), "count": 0}
                failed_sources += 1
            elif result is None:
                # –ö–†–ò–¢–ò–ß–ù–û: None —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—à–∏–±–∫–æ–π
                log_error("aggregator", f"Scraper '{source_name}' –≤–µ—Ä–Ω—É–ª None - —ç—Ç–æ –æ—à–∏–±–∫–∞!")
                source_stats[source_name] = {"error": "–í–µ—Ä–Ω—É–ª None", "count": 0}
                failed_sources += 1
            elif isinstance(result, list):
                count = len(result)
                all_listings.extend(result)
                source_stats[source_name] = {"count": count, "error": None}
                successful_sources += 1
                log_info("aggregator", f"‚úÖ Scraper '{source_name}': –ø–æ–ª—É—á–µ–Ω–æ {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            else:
                log_error("aggregator", f"Scraper '{source_name}': –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {result}")
                source_stats[source_name] = {"error": f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}", "count": 0}
                failed_sources += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        log_info("aggregator", f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: —É—Å–ø–µ—à–Ω–æ {successful_sources}/{len(source_names)}, –æ—à–∏–±–æ–∫ {failed_sources}")
        log_info("aggregator", f"üìä –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(all_listings)}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        unique_listings = self._remove_duplicates(all_listings)
        duplicates_removed = len(all_listings) - len(unique_listings)
        if duplicates_removed > 0:
            log_info("aggregator", f"–£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ID")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ signature (–∞–¥—Ä–µ—Å + vendor + —Ü–µ–Ω–∞ + —Ñ–æ—Ç–æ)
        if unique_listings:
            try:
                from scrapers.aggregator_utils import dedupe_by_signature
                before_signature = len(unique_listings)
                unique_listings = dedupe_by_signature(unique_listings)
                signature_removed = before_signature - len(unique_listings)
                if signature_removed > 0:
                    log_info("aggregator", f"[AGGREGATOR] —É–¥–∞–ª–µ–Ω–æ {signature_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ signature (–∏–∑ {before_signature})")
            except ImportError as e:
                log_warning("aggregator", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å dedupe_by_signature: {e}")
            except Exception as e:
                log_error("aggregator", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ signature: {e}")
        
        # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü—É apartments –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–µ–π
        # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –ë–î, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ –ø–∞–º—è—Ç–∏
        if unique_listings:
            try:
                from database_turso import sync_apartments_batch
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–µ–π
                inserted_ids = await sync_apartments_batch(unique_listings)
                
                if inserted_ids:
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                    new_listings = [
                        listing for listing in unique_listings
                        if str(listing.id) in inserted_ids
                    ]
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ —Ñ–æ–Ω–µ, –Ω–µ –±–ª–æ–∫–∏—Ä—É—è –ø–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                    from bot.services.notification_service import notify_users_about_new_apartments_summary
                    asyncio.create_task(
                        notify_users_about_new_apartments_summary(new_listings)
                    )
                    
                    log_info("aggregator", f"[AGGREGATOR] –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ notify: {len(new_listings)}")
                else:
                    log_info("aggregator", "[AGGREGATOR] –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ—Ç")
            except ImportError as e:
                log_error("aggregator", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å sync_apartments_batch: {e}")
            except Exception as e:
                log_error("aggregator", f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ apartments: {e}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ) - —É –Ω–∞—Å –Ω–µ—Ç –¥–∞—Ç—ã, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ü–µ–Ω–µ
        unique_listings.sort(key=lambda x: x.price if x.price > 0 else 999999999)
        
        log_info("aggregator", f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(unique_listings)}")
        
        # –ö–†–ò–¢–ò–ß–ù–û: —è–≤–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫
        if not isinstance(unique_listings, list):
            log_error("aggregator", f"–û–®–ò–ë–ö–ê: _remove_duplicates –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫: {type(unique_listings)}")
            return []
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º unique_listings –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        # new_apartments –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ –∞—Ç—Ä–∏–±—É—Ç –∏–ª–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        return unique_listings
    
    async def _fetch_from_source(
        self,
        scraper,
        source_name: str,
        city: str,
        min_rooms: int,
        max_rooms: int,
        min_price: int,
        max_price: int,
    ) -> List[Listing]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        –û–±–µ—Ä–Ω—É—Ç –≤ try/except –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–∞–¥–µ–Ω–∏–π.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –æ—Å—Ç–∞–ª—å–Ω—ã–µ scraper'—ã –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç—É.
        
        Args:
            scraper: –≠–∫–∑–µ–º–ø–ª—è—Ä scraper'–∞
            source_name: –ò–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            city: –ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            min_rooms: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
            max_rooms: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
            min_price: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
            max_price: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        scraper_name = getattr(scraper, 'SOURCE_NAME', source_name)
        
        try:
            log_info("aggregator", f"üîÑ –ó–∞–ø—É—Å–∫–∞—é scraper '{scraper_name}' –¥–ª—è –≥–æ—Ä–æ–¥–∞ '{city}'...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scraper'–∞ (context manager)
            try:
                async with scraper:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                    listings = await scraper.fetch_listings(
                        city=city,
                        min_rooms=min_rooms,
                        max_rooms=max_rooms,
                        min_price=min_price,
                        max_price=max_price,
                    )
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –ö–†–ò–¢–ò–ß–ù–û: –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–æ–∫, –Ω–µ None
                    if listings is None:
                        log_error("aggregator", f"Scraper '{scraper_name}' –≤–µ—Ä–Ω—É–ª None –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞ - —ç—Ç–æ –æ—à–∏–±–∫–∞!")
                        return []
                    
                    if not isinstance(listings, list):
                        log_error("aggregator", f"Scraper '{scraper_name}' –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫: {type(listings)}, –∑–Ω–∞—á–µ–Ω–∏–µ: {listings}")
                        return []
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –¥–µ—Ç–∞–ª—è–º–∏
                    count = len(listings)
                    log_info("aggregator", f"‚úÖ Scraper '{scraper_name}': –ø–æ–ª—É—á–µ–Ω–æ {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã - —ç—Ç–æ Listing –æ–±—ä–µ–∫—Ç—ã
                    if count > 0:
                        first_item = listings[0]
                        if not isinstance(first_item, Listing):
                            log_warning("aggregator", f"Scraper '{scraper_name}': –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –Ω–µ Listing, –∞ {type(first_item)}")
                    
                    return listings
                    
            except asyncio.TimeoutError as e:
                log_error("aggregator", f"Scraper '{scraper_name}': —Ç–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö", e)
                return []
            except aiohttp.ClientError as e:
                log_error("aggregator", f"Scraper '{scraper_name}': –æ—à–∏–±–∫–∞ HTTP-–∑–∞–ø—Ä–æ—Å–∞", e)
                return []
            except Exception as e:
                log_error("aggregator", f"Scraper '{scraper_name}': –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å context manager", e)
                return []
                
        except Exception as e:
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è, –∏–º–ø–æ—Ä—Ç –∏ —Ç.–¥.)
            log_error("aggregator", f"Scraper '{scraper_name}': –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", e)
            return []
    
    def _remove_duplicates(self, listings: List[Listing]) -> List[Listing]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        seen_ids = set()
        unique = []
        
        for listing in listings:
            if listing.id not in seen_ids:
                seen_ids.add(listing.id)
                unique.append(listing)
        
        return unique
    
    @classmethod
    def get_available_sources(cls) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return list(cls.SCRAPERS.keys())


async def apartment_dict_to_listing(apartment_dict: Dict[str, Any]) -> Optional[Listing]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∏–∑ —Ç–∞–±–ª–∏—Ü—ã apartments –≤ –æ–±—ä–µ–∫—Ç Listing
    
    Args:
        apartment_dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã apartments
    
    Returns:
        –û–±—ä–µ–∫—Ç Listing –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–Ω—É –∏ –≤–∞–ª—é—Ç—É
        price_usd = apartment_dict.get("price_usd") or 0
        price_byn = apartment_dict.get("price_byn") or 0
        currency = apartment_dict.get("currency", "USD")
        
        # –í—ã–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–Ω—É
        if currency == "USD":
            price = price_usd
        elif currency == "BYN":
            price = price_byn
        else:
            price = price_usd if price_usd > 0 else price_byn
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ü–µ–Ω—É
        if currency == "USD":
            price_formatted = f"${price:,}".replace(",", " ") if price > 0 else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
        else:
            price_formatted = f"{price:,} BYN".replace(",", " ") if price > 0 else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—É –≤ –¥—Ä—É–≥–æ–π –≤–∞–ª—é—Ç–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if price_usd and price_byn:
            if currency == "USD":
                price_formatted += f" ({price_byn:,} BYN)".replace(",", " ")
            else:
                price_formatted += f" (${price_usd:,})".replace(",", " ")
        
        # –ü–æ–ª—É—á–∞–µ–º photos (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø–∏—Å–æ–∫ –∏–ª–∏ JSON —Å—Ç—Ä–æ–∫–∞)
        photos = apartment_dict.get("photos", [])
        if isinstance(photos, str):
            try:
                photos = json.loads(photos) if photos else []
            except:
                photos = []
        if not isinstance(photos, list):
            photos = []
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º title –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        title = apartment_dict.get("title", "")
        if not title:
            rooms = apartment_dict.get("rooms", 0)
            area = apartment_dict.get("total_area", 0.0)
            if rooms and area:
                title = f"{rooms}-–∫–æ–º–Ω., {area} –º¬≤"
            else:
                title = "–ö–≤–∞—Ä—Ç–∏—Ä–∞"
        
        listing = Listing(
            id=apartment_dict.get("ad_id", ""),
            source=apartment_dict.get("source", "unknown"),
            title=title,
            price=price,
            price_formatted=price_formatted,
            rooms=apartment_dict.get("rooms", 0),
            area=apartment_dict.get("total_area", 0.0),
            address=apartment_dict.get("address", ""),
            url=apartment_dict.get("url", ""),
            photos=photos,
            floor=apartment_dict.get("floor", ""),
            description=apartment_dict.get("description", ""),
            currency=currency,
            price_usd=price_usd,
            price_byn=price_byn,
            year_built=apartment_dict.get("year_built", ""),
            created_at=apartment_dict.get("created_at", ""),
            is_company=apartment_dict.get("is_company"),
            balcony=apartment_dict.get("balcony", ""),
            bathroom=apartment_dict.get("bathroom", ""),
            total_floors=apartment_dict.get("total_floors", ""),
            house_type=apartment_dict.get("house_type", ""),
            renovation_state=apartment_dict.get("renovation_state", ""),
            kitchen_area=apartment_dict.get("kitchen_area", 0.0),
            living_area=apartment_dict.get("living_area", 0.0),
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º raw_json –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        if "raw_json" in apartment_dict:
            listing.raw_json = apartment_dict["raw_json"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º city –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
        if "city" in apartment_dict:
            listing.city = apartment_dict["city"]
        
        return listing
    except Exception as e:
        log_error("aggregator", f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ apartment_dict –≤ Listing: {e}")
        return None


# –ü–æ—Ä–æ–≥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º (–≤ –º–µ—Ç—Ä–∞—Ö)
GEO_THRESHOLD_METERS = 80


def _extract_coords_from_listing(listing: Listing) -> tuple[Optional[float], Optional[float]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑:
    1. –ê—Ç—Ä–∏–±—É—Ç–æ–≤ listing.lat –∏ listing.lon (–µ—Å–ª–∏ –µ—Å—Ç—å)
    2. raw_json (–µ—Å–ª–∏ –µ—Å—Ç—å –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç coordinates)
    
    Args:
        listing: –û–±—ä—è–≤–ª–µ–Ω–∏–µ
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (lat, lon) –∏–ª–∏ (None, None) –µ—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –æ–±—ä–µ–∫—Ç–∞
    lat = getattr(listing, "lat", None)
    lon = getattr(listing, "lon", None)
    
    if lat is not None and lon is not None:
        return (lat, lon)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ raw_json (–µ—Å–ª–∏ –µ—Å—Ç—å)
    raw_json = getattr(listing, "raw_json", None)
    if raw_json:
        try:
            import json
            if isinstance(raw_json, str):
                data = json.loads(raw_json)
            else:
                data = raw_json
            
            # Kufar API —Ö—Ä–∞–Ω–∏—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫–∞–∫ [lon, lat]
            coords = data.get("coordinates")
            if coords and isinstance(coords, list) and len(coords) >= 2:
                lon, lat = coords[0], coords[1]
                return (lat, lon)
        except Exception:
            pass
    
    return (None, None)


def _extract_city_from_listing(listing: Listing) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –≥–æ—Ä–æ–¥ –∏–∑:
    1. –ê—Ç—Ä–∏–±—É—Ç–∞ listing.city (–µ—Å–ª–∏ –µ—Å—Ç—å)
    2. –ê–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ _extract_city_from_address
    
    Args:
        listing: –û–±—ä—è–≤–ª–µ–Ω–∏–µ
    
    Returns:
        –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
    """
    city = getattr(listing, "city", None)
    if city:
        return str(city).strip().lower()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –∞–¥—Ä–µ—Å–∞
    from database_turso import _extract_city_from_address
    return _extract_city_from_address(listing.address or "").lower()


def extract_vendor_from_listing(listing: Listing) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç vendor (agency –∏–ª–∏ seller) –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    
    –ü—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å vendor –∏–∑:
    1. –ê—Ç—Ä–∏–±—É—Ç–∞ listing.vendor (–µ—Å–ª–∏ –µ—Å—Ç—å)
    2. raw_json (agency –∏–ª–∏ seller)
    
    Args:
        listing: –û–±—ä—è–≤–ª–µ–Ω–∏–µ
    
    Returns:
        –ù–∞–∑–≤–∞–Ω–∏–µ vendor –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç listing.vendor
    vendor = getattr(listing, "vendor", None)
    if vendor:
        return str(vendor).strip()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ raw_json
    raw_json = getattr(listing, "raw_json", None)
    if raw_json:
        try:
            if isinstance(raw_json, dict):
                vendor = raw_json.get("agency") or raw_json.get("seller")
            elif isinstance(raw_json, str):
                data = json.loads(raw_json)
                vendor = data.get("agency") or data.get("seller")
            else:
                vendor = None
            
            if vendor:
                return str(vendor).strip()
        except Exception:
            pass
    
    return None


def make_group_key(listing: Listing) -> tuple:
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–ª—é—á –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
    1. –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –¥–æ–º–∞ -> (house_key, city, street, house) –∏–ª–∏ (house_vendor_key, city, street, house, vendor)
    2. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã -> (coords_key, city, street, rounded_lat, rounded_lon) –∏–ª–∏ (coords_vendor_key, city, street, lat, lon, vendor)
    3. –ò–Ω–∞—á–µ -> (street_key, city, street)
    
    –ï—Å–ª–∏ GROUP_BY_VENDOR_FOR_ADDRESS=True –∏ –µ—Å—Ç—å vendor, –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è vendor –≤ –∫–ª—é—á.
    
    Args:
        listing: –û–±—ä—è–≤–ª–µ–Ω–∏–µ
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂-–∫–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    """
    from utils.address_utils import split_address
    from config import GROUP_BY_VENDOR_FOR_ADDRESS
    
    addr = split_address(listing.address or "")
    city = _extract_city_from_listing(listing)
    street = addr["street"]
    house = addr["house"]
    vendor = extract_vendor_from_listing(listing) if GROUP_BY_VENDOR_FOR_ADDRESS else None
    
    if house:
        if GROUP_BY_VENDOR_FOR_ADDRESS and vendor:
            return ("house_vendor_key", city, street, house, vendor)
        return ("house_key", city, street, house)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    lat, lon = _extract_coords_from_listing(listing)
    if lat is not None and lon is not None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–∫—Ä—É–≥–ª–µ–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—ã–π bucket
        rounded_lat = round(lat, 4)
        rounded_lon = round(lon, 4)
        if GROUP_BY_VENDOR_FOR_ADDRESS and vendor:
            return ("coords_vendor_key", city, street, rounded_lat, rounded_lon, vendor)
        return ("coords_key", city, street, rounded_lat, rounded_lon)
    
    return ("street_key", city, street)


def group_similar_listings(listings: List[Listing]) -> List[List[Listing]]:
    """
    –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ –∞–¥—Ä–µ—Å—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
    
    –û–±—ä—è–≤–ª–µ–Ω–∏—è —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–º–Ω–∞—Ç –≤ –æ–¥–Ω–æ–º –¥–æ–º–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é:
    1. –ü–µ—Ä–≤–∏—á–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É (–¥–æ–º/–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã/—É–ª–∏—Ü–∞) - –ë–ï–ó —É—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
    2. –ì–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ (distance-based)
    
    Args:
        listings: –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ - —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π)
    """
    from utils.geo import haversine_m
    
    # 1) –ü–µ—Ä–≤–∏—á–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É
    buckets = defaultdict(list)
    for l in listings:
        key = make_group_key(l)
        buckets[key].append(l)
    
    # 2) –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ coords_key –¥–µ–ª–∞–µ–º —Ç–æ—á–Ω–æ–µ –≥–µ–æ-–∫–ª–∞—Å—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ (distance-based)
    final_groups = []
    for key, bucket in buckets.items():
        tag = key[0]
        if tag != "coords_key" or len(bucket) <= 1:
            final_groups.append(bucket)
            continue
        
        # –ê–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é (O(n^2) –≤ bucket'–µ, bucket –æ–±—ã—á–Ω–æ –º–∞–ª)
        used = [False] * len(bucket)
        for i, a in enumerate(bucket):
            if used[i]:
                continue
            group = [a]
            used[i] = True
            
            lat_a, lon_a = _extract_coords_from_listing(a)
            if lat_a is None or lon_a is None:
                continue
            
            for j in range(i+1, len(bucket)):
                if used[j]:
                    continue
                b = bucket[j]
                lat_b, lon_b = _extract_coords_from_listing(b)
                if lat_b is None or lon_b is None:
                    continue
                
                d = haversine_m(lat_a, lon_a, lat_b, lon_b)
                if d <= GEO_THRESHOLD_METERS:
                    group.append(b)
                    used[j] = True
            
            final_groups.append(group)
    
    return final_groups


async def notify_users_about_new_apartments(new_listings: List[Listing]) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –æ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö
    
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤ —Ñ–æ–Ω–µ –∏ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏ (—É–∂–µ —Ä–µ–∞–ª—å–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≤ –ë–î).
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç sent_ads –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∑–∞—â–∏—Ç—ã –æ—Ç –¥—É–±–ª–µ–π.
    
    Args:
        new_listings: –°–ø–∏—Å–æ–∫ Listing –æ–±—ä–µ–∫—Ç–æ–≤ - —Ä–µ–∞–ª—å–Ω–æ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π (—É–∂–µ –≤ –ë–î)
    """
    if not new_listings:
        log_info("aggregator", "[NOTIFY] –Ω–µ—Ç –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π")
        return
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
        from database import get_active_users, get_user_filters
        from bot.services.search_service import _process_user_listings_normal_mode, validate_user_filters, matches_user_filters
        from bot.services.ai_service import check_new_listings_ai_mode
        from database import is_ad_sent_to_user
        from aiogram import Bot
        from config import BOT_TOKEN as TELEGRAM_BOT_TOKEN
        
        if not TELEGRAM_BOT_TOKEN:
            log_warning("aggregator", "[NOTIFY] TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            return
        
        log_info("aggregator", f"[NOTIFY] –Ω–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(new_listings)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        active_users = await get_active_users()
        if not active_users:
            log_info("aggregator", "[NOTIFY] –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            return
        
        log_info("aggregator", f"[NOTIFY] –Ω–∞–π–¥–µ–Ω–æ {len(active_users)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º –±–æ—Ç
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        try:
            total_sent = 0
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ –µ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞–º
            for user_id in active_users:
                try:
                    user_filters = await get_user_filters(user_id)
                    if not user_filters:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–æ–≤
                    is_valid, error_msg = validate_user_filters(user_filters)
                    if not is_valid:
                        continue
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫ –Ω–æ–≤—ã–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º
                    filtered_listings = []
                    for listing in new_listings:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –ª–∏ —É–∂–µ —ç—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (sent_ads - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞)
                        if await is_ad_sent_to_user(user_id, listing.id):
                            continue
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                        if matches_user_filters(listing, user_filters, user_id=user_id, log_details=False):
                            filtered_listings.append(listing)
                    
                    if not filtered_listings:
                        continue
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ –∞–¥—Ä–µ—Å—É –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–º–Ω–∞—Ç
                    groups = group_similar_listings(filtered_listings)
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
                    if user_filters.get("ai_mode"):
                        # –í –ò–ò-—Ä–µ–∂–∏–º–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ —Ä–∞–Ω—å—à–µ
                        await check_new_listings_ai_mode(bot, user_id, user_filters, filtered_listings)
                    else:
                        # –í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
                        from bot.services.notification_service import send_listing_to_user, send_grouped_listings_to_user
                        
                        user_sent = 0
                        for group in groups:
                            if len(group) == 1:
                                # –û–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
                                result = await send_listing_to_user(bot, user_id, group[0], use_ai_valuation=False)
                                if result:
                                    user_sent += 1
                            else:
                                # –ù–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                                result = await send_grouped_listings_to_user(bot, user_id, group)
                                if result:
                                    user_sent += len(group)
                        
                        total_sent += user_sent
                        
                except Exception as e:
                    log_error("aggregator", f"[NOTIFY] –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
                    continue
            
            log_info("aggregator", f"[NOTIFY] –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {total_sent} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
        finally:
            await bot.session.close()
        
    except ImportError as e:
        log_error("aggregator", f"[NOTIFY] –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏: {e}")
    except Exception as e:
        log_error("aggregator", f"[NOTIFY] –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {e}")
        import traceback
        traceback.print_exc()


async def test_aggregator():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...\n")
    
    aggregator = ListingsAggregator()
    
    listings = await aggregator.fetch_all_listings(
        city="–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏",
        min_rooms=1,
        max_rooms=3,
        min_price=0,
        max_price=50000,
    )
    
    print(f"\n{'='*50}")
    print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
    print(f"{'='*50}\n")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    for i, listing in enumerate(listings[:5], 1):
        print(f"--- –û–±—ä—è–≤–ª–µ–Ω–∏–µ {i} ---")
        print(f"üè∑Ô∏è  –ò—Å—Ç–æ—á–Ω–∏–∫: {listing.source}")
        print(f"üè† {listing.title}")
        print(f"üí∞ –¶–µ–Ω–∞: {listing.price_formatted}")
        print(f"üö™ –ö–æ–º–Ω–∞—Ç: {listing.rooms}")
        print(f"üìê –ü–ª–æ—â–∞–¥—å: {listing.area} –º¬≤")
        print(f"üìç –ê–¥—Ä–µ—Å: {listing.address}")
        print(f"üîó URL: {listing.url}")
        print(f"üì∏ –§–æ—Ç–æ: {len(listing.photos)} —à—Ç.")
        print()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    from collections import Counter
    sources = Counter(l.source for l in listings)
    for source, count in sources.most_common():
        print(f"  ‚Ä¢ {source}: {count}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
    parser.add_argument("--city", type=str, default="–±–∞—Ä–∞–Ω–æ–≤–∏—á–∏", help="–ì–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞")
    parser.add_argument("--min-rooms", type=int, default=1, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç")
    parser.add_argument("--max-rooms", type=int, default=4, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç")
    parser.add_argument("--min-price", type=int, default=0, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞")
    parser.add_argument("--max-price", type=int, default=100000, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞")
    parser.add_argument("--max-pages", type=int, default=None, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–ª—è —Å–∫—Ä–µ–π–ø–µ—Ä–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö —ç—Ç–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä)")
    
    args = parser.parse_args()
    
    async def run_aggregator():
        """–ó–∞–ø—É—Å–∫ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...\n")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        print(f"  –ì–æ—Ä–æ–¥: {args.city}")
        print(f"  –ö–æ–º–Ω–∞—Ç—ã: {args.min_rooms}-{args.max_rooms}")
        print(f"  –¶–µ–Ω–∞: ${args.min_price:,}-${args.max_price:,}".replace(",", " "))
        if args.max_pages:
            print(f"  –ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü: {args.max_pages}")
        print()
        
        aggregator = ListingsAggregator()
        
        listings = await aggregator.fetch_all_listings(
            city=args.city,
            min_rooms=args.min_rooms,
            max_rooms=args.max_rooms,
            min_price=args.min_price,
            max_price=args.max_price,
        )
        
        print(f"\n{'='*50}")
        print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(listings)}")
        print(f"{'='*50}\n")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        for i, listing in enumerate(listings[:5], 1):
            print(f"--- –û–±—ä—è–≤–ª–µ–Ω–∏–µ {i} ---")
            print(f"üè∑Ô∏è  –ò—Å—Ç–æ—á–Ω–∏–∫: {listing.source}")
            print(f"üè† {listing.title}")
            print(f"üí∞ –¶–µ–Ω–∞: {listing.price_formatted}")
            print(f"üö™ –ö–æ–º–Ω–∞—Ç: {listing.rooms}")
            print(f"üìê –ü–ª–æ—â–∞–¥—å: {listing.area} –º¬≤")
            print(f"üìç –ê–¥—Ä–µ—Å: {listing.address}")
            print(f"üîó URL: {listing.url}")
            print(f"üì∏ –§–æ—Ç–æ: {len(listing.photos)} —à—Ç.")
            print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        from collections import Counter
        sources = Counter(l.source for l in listings)
        for source, count in sources.most_common():
            print(f"  ‚Ä¢ {source}: {count}")
    
    asyncio.run(run_aggregator())

